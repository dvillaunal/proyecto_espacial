---
title: | 
  <center> Geoestadística: Temperatura </center>
  <center> Estadística espacial </center>
author: | 
  <center> Jhonatan Smith Garcia Muñoz </center>
  <center> Julian Gallego Villa </center>
  
date: "Mayo, 2022"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Sobre la temperatura

**Algunos paquetes de interes**
```{r message=FALSE, warning=FALSE}
library(geoR); library(gstat) ;library(sf);library(raster)
library(rgdal)
library(vegan)
library(ade4)
set.seed(123)

```
``
Aqui claramente se ve que el tipo de dato es un "MULTIPOLYGON". Algo esperable. 

```{r}
plot(col$geometry)
```
Ahora, para el ejercicio de interes, se focalizará la informacion en un 
departamento especifico. Para este caso, Santander (Se seleccionó aleatoreamente).

```{r}
santander <- col[31,"geometry"]
borderland <- st_coordinates(santander) #matriz del sistema coordenadas
borderland <- as.data.frame(borderland[,1:2])

```

¿Que se hizo en este paso? 



# Cargue base de datos de interes

Sacado de : https://swat.tamu.edu/data/

La pagina suministra 2 archivos con 2 bases de datos diferentes. Una con datos
de la precipitacion y otro de la temperatura. 


Ahora, con ambas bases de datos se pueden realizar preguntas de diversos tipos.

**Se escoje una fecha para trabajar**

Debido a que no se trabajara con problemas espacio-temporales se seleccionó de manera arbitrarea una fecha. Para este ejercicio se toman los datos de la manera ya mencionada para el 27-08-09


```{r}

data <- read.csv2("datos_smith.csv")

head(data)

```

# Sobre la autocorrelacion

Para el correcto analisis espacial es necesario probar autocorrelacion espacial. Se procede a analizar las pruebas pertinentes.

#Conversion de coordenadas geograficas (LongLat) a coordenadas planas (UTM)



```{r}
attach(data)

datos <- as.matrix(data.frame(x=as.numeric(data$lat),
                              y=as.numeric(data$long))) # se obtienen las coordenadas en longitud y latitud

head(datos)

```

**Creacion Dataframe Espacial**

```{r}

x1 <- as.numeric(data$tmin)
x2 <- as.numeric(data$tmax)
x3 <- as.numeric(data$elev)
df <- cbind(datos, x1,x2,x3)
df <- data.frame(df)
colnames(df)<-c("X","Y", "Temp min", "Temp max", "elev")
```


```{r}
geo_temp.max <- as.geodata(df, coords = 1:2, data.col = 4, borders=TRUE)
geo_temp.max$borderland<- borderland
datos.sp <- df
coordinates(datos.sp) <- ~X+Y
```

```{r}
plot(geo_temp.max, lowess=TRUE)
```



# Prueba de normalidad

```{r}
qqnorm(df$`Temp max`)
qqline(df$`Temp max`)
```

$$ H_{0}: \text{ La temperatura maxima distribuye de forma normal} $$

$$ H_{1}: \text{ La temperatura maxima NO distribuye de forma normal}  $$

```{r}
shapiro.test(df$`Temp max`)
```

Dado que el valor p es de 0.02413, es tiene que la teperatura maxima en Santander registrada el 27 de agosto del 2009 no distribuye normal. 

Tenga presente que la muestra es de tamaño 24 y lo mas probable es que la prueba de normalidad se puede ver afectada por dicho tamaño (n pequeño); dado este caso, no se deberia de continuar con los analisis puesto que se tiene un supuesto de normalidad. Ahora, las puebas aqui presentes son robustas a desviaciones ligeras de normalidad y, ademas se tiene lo siguiente:

**Otros datos**

Suponga los datos de otra fecha para la temperatura maxima. 

```{r}
prueba <- read.csv("datos_ambientales.csv", header=TRUE)
```

Dichos datos corresponden a las temperaturas de Santander el 31 de julio del 2014. Se extraen los datos de la misma manera que ya se mencionó solo que, para esta fecha, la base de datos ya poseia mas registros.

Ahora, notar que:

```{r}
shapiro.test(prueba$max.temperature)
```
Al realizar la misma prueba de hipotesis sobre la variable de temperatura maxima pero de otra fecha (seleccionada con un tamaño de muestra de n>30) se tinen datos normales.

Si bien, esto no representa una prueba formal acerca de la normaldiad de los datos, lo que se desea enfatizar es que, a pesar de que el resultado de la primer muestra seleccionada ha rechazado el supuesto de normalida de los datos, un fenomeno natural como la temperatura cabria esperar, por la naturaleza de su proceso, que siguiese un proceso normal. Dicha informacion puede ser constatada en otros articulos pero esta no es la preocupacion principal del presente.

Por ahora, se hace mencion a que, si se selecciona una muestra de tamaño adecuado, en general, la hipotesis de normalidad acerca de la temperatura maxima en general se cumple.

Es por esto que, para este caso,se asume que a pesar de la no normaldiad presentada por el test de Shapiro-Wilk, se procede con los analisis estadisticos-espaciales.

A partir de este momento, se realizan analisis para dos fechas, dado que se busca demostrar las tecnicas vistas en clase, se plantea entonces analisar los datos propuestos inicialmente y los datos para la fecha que si cumple los supuestos, esto con la intencion de ver como el tamaño de muestra y la desviacion de la normalidad podria afectar los resultados

```{r}
df2 <- prueba
```

 
```{r}

distancia1 <- dist(df[,1:2], diag=TRUE, upper=TRUE)

dif.temp1 <- dist(df[,4], diag=TRUE, upper=TRUE)^2

distancia2 <- dist(df2[,1:2], diag=TRUE, upper=TRUE)

dif.temp2 <- dist(df2[,4], diag=TRUE, upper=TRUE)^2


```
 
# Grafico distancia vs disimilitud (27 de agosto del 2009)
 
```{r}

datosmatriz1 <- data.frame(dif.prof=dif.temp1[lower.tri(dif.temp1)], distancia1=distancia1[lower.tri(distancia1)])

```
 

 
# Grafico distancia vs disimilitud (31 de julio del 2014)

```{r}


datosmatriz2 <- data.frame(dif.prof=dif.temp2[lower.tri(dif.temp2)], distancia2=distancia2[lower.tri(distancia2)])

```


```{r}
library(ggplot2)
graph <- ggplot(datosmatriz2, aes(x=distancia2, y=dif.temp2))+
                 geom_point() +theme_test(base_size=14)+
                  labs(x="Distancia (m)", y="Diferencia en temperatura (2014)")+
  theme(axis.title=element_text(face="bold"), axis.text= element_text(color="black"))+
                   geom_smooth(method=lm)
graph

```
 
 Aunque no es muy marcado, se observa que, a distancias mas pequeñas, la disimilitud es mas pequeña. Se podria pensar existe una autocorrelacion espacial.
 

Segun este metodo a una significancia de 0.05 se rechaza la hipotesis nula acerca de aleatoriedad espacial. Entonces se concluye hay autocorrelacion espacial.

```{r}

mantel.rtest(distancia1, dif.temp1, nrepet=999)

```

 Con el test de Mantel se prueba que existe autocorrelacion espacial; es decir,
 la temperatura es similar entre unidades geograficas proximas.

**Para datos del 2014**


```{r}
mantel(distancia2, dif.temp2)
```

```{r}
mantel.rtest(distancia2, dif.temp2)
```
Como era de esperarse, para ambos test, exise evidencia para con la autocorrelacion espacial. 
 
 Como en ambas fechas, la hipotesis de autocorrelacion espacial no se vio afectada por la desviacion ligera de normalidad, entonces se procede como se propuso inicialmente y se analizará la temperatura maxima en santander para la fecha seleccionada en el 2009.
 
```{r}
resumen <- summary(geo_temp.max)
distancia <- resumen$distances.summary
dm <- distancia[2]/2; dm
``` 

```{r message=FALSE, warning=FALSE}
vario1 <- variog(geo_temp.max, option = "cloud") 
vario2 <- variog(geo_temp.max, option = "cloud", max.dist = dm)
```

```{r fig.height=5, fig.width=12, message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
plot(vario1, pch=21, bg="blue", lwd="black",
     main="Variograma sin ajuste con regla empírica")
plot(vario2, pch=21, bg="red", lwd="black",
     main="Variograma ajuste con regla empírica")
```

Ambos semivariogramas refuerzan la hipótesis inicial respecto a la correlación
espacial, lo anterior es fácil de observar en el gráco de la derecha pues para
distancias pequeñas la semivarianza tiene valores más pequeños que para
distancias intermedias y grandes, se observa un comportamiento en alguna
medida “creciente” respecto a las distancias.

# Variogramas tipo bin

```{r}
bin1 <- variog(geo_temp.max,option="bin")
bin2 <- variog(geo_temp.max,option="bin",max.dist=dm)
```


```{r fig.height=5, fig.width=12, message=FALSE, warning=FALSE}
par(mfrow=c(1,2))
plot(bin1, pch=21, bg="blue", lwd="black", main="Semivariograma sin ajuste con regla empírica")
plot(bin2, pch=21, bg="red", lwd="black",
     main="Semivariograma ajuste con regla empírica")
```

Que cosa mas fea... Pero, asi es la vida. 

Se puede notar a simplevista que los semivariogramas no siguen una estructura de los modelos teoricos vistos. La idea será finalmente, tratar de ajustar uno con una medida aceptable par aeralizar prediccions.

**Autocorrelacion con semivariograma**

```{r}
library(sm)
sm.variogram(datos.sp@coords, datos.sp@data$`Temp max`,model="independent")
```

Este semivariograma apunta a que no existe autocorrelacion espacial puesto que todos los puntos del semivariograma estan dentro de las bandas de confianza. Aun asi, la prueba formal ya comprobó que si existe dicha autocorrelacion. Sin embargo, se debe de realziar entonces una verificacion de los datos y sospechar que, en efecto, el tamaño de la muestra y la prueba de normalidad quizas esten afectando en analisis descriptivo.

```{r}
ini.vals1 <- c(17.79,0.7)
```

```{r}
fit_matern <- variofit(vario = bin2,ini.cov.pars = ini.vals1, fix.nugget =F,
                   weights = "npairs", min="optim", nugget = 0.03)
```

```{r message=FALSE, warning=FALSE}
plot(bin2, pch=21, bg="blue", lwd="black",
     main="Semivariograma con ajuste Matern")
lines(fit_matern)
```


```{r}
library (gstat)
library(MASS)
```


# Modelos considerados

## Matern

Valores iniciales

```{r}
ini.vals1 <- c(17.79,0.7)
```

```{r}
fit_matern <- variofit(vario = bin2,ini.cov.pars = ini.vals1, fix.nugget =F,
                   weights = "npairs", min="optim", nugget = 0.03)
```

```{r message=FALSE, warning=FALSE}
plot(bin2, pch=21, bg="red", lwd="black",
     main="Semivariograma con ajuste matern")
lines(fit_matern)
```
## Cubico


```{r}
ini.vals2 <- c(15.61,1.58)
```

```{r}
fit_cubic <- variofit(vario = bin2,ini.cov.pars = ini.vals2, fix.nugget = TRUE,
                   weights = "npairs", min="optim", nugget = 0.03, cov.model = "cubic")
```

```{r message=FALSE, warning=FALSE}
plot(bin2, pch=21, bg="red", lwd="black",
     main="Semivariograma con ajuste cubico")
lines(fit_cubic)
```

```{r}
library(knitr)
ss_cubic <- summary(fit_cubic)$sum.of.squares
ss_matern <- summary(fit_matern)$sum.of.squares
t_ss_m <- rbind(c(ss_cubic, ss_matern))
t_ss_m <- as.data.frame(t_ss_m)
colnames(t_ss_m) <- c("Modelo cubico","Modelo Matern")
rownames(t_ss_m) <- "SS"
kable(t_ss_m)
```

Se seguirá el ajuste haciendo uso del modelo Matern. 

## Ajuste de pesos clásicos

```{r}
ini.vals <- c(17.79,0.7)
```

```{r}
fit_clasicos <- variofit(vario = bin2,ini.cov.pars = ini.vals, fix.nugget =F,
                   weights = "npairs", min="optim", nugget = 0.03)
```
 
```{r message=FALSE, warning=FALSE}
plot(bin2, pch=21, bg="red", lwd="black",
     main="Semivariograma ajustado con pesos clasicos")
lines(fit_clasicos)
```

Suma de cuadrados

```{r}
sum_clas <- summary(fit_clasicos)
SSclas <- sum_clas$sum.of.squares
RMSE_clasicos <-mean(sqrt(SSclas))
RMSE_clasicos
```



## Ajuste de pesos por mínimos cuadrados ordinarios

```{r}
fit_minimos <- variofit(vario = bin2,ini.cov.pars = ini.vals, fix.nugget =F,
                   weights = "equal", min="optim")
```


```{r warning=FALSE}
plot(bin2, pch=21, bg="blue", lwd="black",
     title="Semivariograma ajustado con minimos cuadrados ordinarios")
lines(fit_minimos)
```
Suma de cuadrados

```{r}
sum_min <- summary(fit_minimos)
SSmin <- sum_min$sum.of.squares
RMSE_minimos <- mean(sqrt(SSmin))
RMSE_minimos
```


## Ajuste por pesos ponderados

```{r}
fit_ponderados <- variofit(vario = bin2,ini.cov.pars = ini.vals, fix.nugget =F,
                   weights = "cressie", min="optim")
```


```{r warning=FALSE}
plot(bin2, pch=21, bg="purple", lwd="black",
     main="Semivariograma ajustado con pesos ponderados")
lines(fit_ponderados)
```
Suma de cuadrados

```{r}
sum_pond <- summary(fit_ponderados)
SSpond <- sum_pond$sum.of.squares
RMSE_ponderados <- mean(sqrt(SSpond))
RMSE_ponderados
```

## Ajuste via máxima verosimilitud

```{r}
fit_vero <- likfit(geo_temp.max,ini.cov.pars=ini.vals,fix.nugget=TRUE,
              fix.kappa=TRUE,lik.method="ML")
```

```{r message=FALSE, warning=FALSE}
plot(bin2,xlab="Distancia", ylab="Semivarianza", 
     pch=21, bg="green", lwd="black", 
     main="Ajuste por maxima verosimilitud")
lines(fit_vero)
```

## Ajuste por máxima verosimilitud restringida

```{r}
fit_verorest <- likfit(geo_temp.max,ini.cov.pars=ini.vals,fix.nugget=TRUE,
             fix.kappa=TRUE,lik.method="RML")
```

```{r message=FALSE, warning=FALSE}
plot(bin2,xlab="Distancia", ylab="Semivarianza", 
     pch=21, bg="green", lwd="black", 
     main="Ajuste por maxima verosimilitud restringida")
lines(fit_verorest)
```

# Selección del mejor ajuste vía sumas cuadráticas


```{r}
t_ss <- rbind(c(RMSE_clasicos, RMSE_minimos, RMSE_ponderados))
t_ss <- as.data.frame(t_ss)
colnames(t_ss) <- c("Pesos clasicos","Minimos cuadrados","Pesos ponderados")
rownames(t_ss) <- "RMSE"
kable(t_ss)
```

Al revisar las sumas cuadráticas de los errores, se observa que el ajuste con menor SS es el conseguido a través del ajuste de pesos mediante pesos ponderados.

Ahora, la seleccion del modelo dependerá unica y exclusivamente del valor del MSE mas pequeño. Para este caso, se seleccionarán varios modelos y se realizarán varios ajustes. No solo los mencionados anteriormente. Con esto, se hará validacion cruzada para constatar cual será el mejor modelo con el que trabajar. 


# Validación cruzada


## Para el ajuste por pesos clásicos

```{r}
cv_clasicos <- xvalid(geo_temp.max, model = fit_clasicos)
```



```{r message=FALSE, warning=FALSE}
lm_clasicos <- lm(cv_clasicos$predicted~cv_clasicos$data)
plot(cv_clasicos$data,cv_clasicos$predicted,
     xlab="Valores observados",ylab="Valores predichos",
     main="Ajuste de las predicciones: Pesos clasicos", pch=21,
     bg="blue", lwd="black")
abline(lm_clasicos,col=2)
```
```{r}
summary(lm_clasicos)
```


## Para el ajuste de pesos por mínimos cuadrados

```{r}
cv_minimos <- xvalid(geo_temp.max, model = fit_minimos)
```


```{r message=FALSE, warning=FALSE}
lm_minimos <- lm(cv_minimos$predicted~cv_minimos$data)
plot(cv_minimos$data,cv_minimos$predicted,
     xlab="Valores observados",ylab="Valores predichos",
     main="Ajuste de las predicciones: Minimos cuadrados", pch=21,
     bg="blue", lwd="black")
abline(lm_minimos,col=2)
```

## Para el ajuste de pesos ponderados

```{r}
cv_ponderados <- xvalid(geo_temp.max, model = fit_ponderados)
```


```{r message=FALSE, warning=FALSE}
lm_ponderados <- lm(cv_ponderados$predicted~cv_ponderados$data)
plot(cv_ponderados$data,cv_ponderados$predicted,
     xlab="Valores observados",ylab="Valores predichos",
     main="Ajuste de las predicciones: Pesos ponderados", pch=21,
     bg="blue", lwd="black")
abline(lm_ponderados,col=2)
```


## Para el ajuste por máxima verosimilitud 

```{r}
cv_vero <- xvalid(geo_temp.max, model = fit_vero)
```

```{r message=FALSE, warning=FALSE}
lm_vero <- lm(cv_vero$predicted~cv_vero$data)
plot(cv_vero$data,cv_vero$predicted,
     xlab="Valores observados",ylab="Valores predichos",
     main="Ajuste de las predicciones: Pesos clasicos", pch=21,
     bg="blue", lwd="black")
abline(lm_vero,col=2)
```

## Para el ajuste por máxima verosimilitud restriginda

```{r}
cv_verorest <- xvalid(geo_temp.max, model = fit_verorest)
```

```{r message=FALSE, warning=FALSE}
lm_verorest <- lm(cv_verorest$predicted~cv_verorest$data)
plot(cv_verorest$data,cv_verorest$predicted,
     xlab="Valores observados",ylab="Valores predichos",
     main="Ajuste de las predicciones: Pesos clasicos", pch=21,
     bg="blue", lwd="black")
abline(lm_verorest,col=2)
```



# Elección de mejor ajuste via RMSE

```{r}

fit_clasicos$
RMSE_clasicos <- mean(sqrt(sum(cv_clasicos$predicted-cv_clasicos$data)^2))

RMSE_minimos <- mean(sqrt(sum(cv_minimos$predicted-cv_minimos$data)^2))

RMSE_ponderados <- mean(sqrt(sum(cv_ponderados$predicted-cv_ponderados$data)^2))

RMSE_vero <- mean(sqrt(sum(cv_vero$predicted-cv_vero$data)^2))

RMSE_verorest <- mean(sqrt(sum(cv_verorest$predicted-cv_verorest$data)^2))

t_RMSE <- rbind(c(RMSE_clasicos, RMSE_minimos, RMSE_ponderados,
                  RMSE_vero, RMSE_verorest))
t_RMSE <- as.data.frame(t_RMSE)
colnames(t_RMSE) <- c("Pesos clasicos","Minimos cuadrados","Pesos ponderados",
                      "Maxima verosimilitud", "Verosimilitud restringida")
rownames(t_RMSE) <- "RMSE"
kable(t_RMSE)
```


Con esto, se tiene que el mejor modelo para realizar predicciones será el de Maxima Verosimilitud.


# Predicción: Kriging ordinario y universal


```{r}
p <- Polygon(borderland)
ps <- Polygons(list(p),1)
sps <- SpatialPolygons(list(ps))
plot(sps)
ambientales <- data
data <- data.frame(max.temparature=30)
spdf <- SpatialPolygonsDataFrame(sps,data) 
```

```{r}
grd <- makegrid(spdf, n = 10000)
colnames(grd) <- c('x','y')
grd_pts <- SpatialPoints(coords = grd, 
                         proj4string=CRS(proj4string(spdf)))
# Filtra los puntos para dejar los que se encuentran dentro del poligono
grd_pts_in <- grd_pts[spdf, ]
```

```{r}
gdf <- as.data.frame(coordinates(grd_pts_in)) 

ggplot(gdf) +
  geom_point(aes(x=x,y=y))

```



## Kriging ordinario


```{r}
# datos.sp es el dataframe orignial <-> geotem.max
# dm es la regla empirica de la distancia

variog2 <- variogram(`Temp max`~X+Y, data = datos.sp, cutoff = dm) # LISTO


mod_ols <- fit.variogram(variog2, model = vgm(model = "Mat",
                                                nugget= 0.3,
                                                psill = 56.166,
                                                range = 2.76), 
                                 fit.method = 6) # Listo

# gdf tiene coordenadas x,y (minusculas)

pred_krig <- krige(`Temp max`~X+Y, ~x+y,df, gdf, model=mod_ols)
head(pred_krig)
```


```{r}

 # geo.datoscte <-> geo.ma.temp (o algo asi)
# Kriging ordinario (media constantes)
kg <- krige.conv(geo_temp.max, locations = gdf, borders = borderland, krige=krige.control(nugget = 0.3, cov.pars = c(sigmasq=56.166, phi= 2.764)))

```

```{r}
# Predicción
par(mfrow=c(1,2))
plot(borderland,main="Kriging ordinario",xlab="East",ylab="North")
image(datoscte.kc,add=TRUE)
contour(datoscte.kc,add=TRUE,drawlabels=TRUE)


```


```{r}
puntos_medicion <- SpatialPoints(coords = ambientales[,1:2], 
                         proj4string=CRS(proj4string(spdf)))
# Filtra las estaciones que se encuentran dentro del polígono
puntos_medicion_in <- puntos_medicion[spdf, ]
estaciones <- as.data.frame(coordinates(puntos_medicion_in)) 
```


```{r}
p <- ggplot(aes(x,y),data=pred_krig)
p <- p +  geom_point(aes(color=var1.pred))
p <- p + geom_point(aes(x,y), data = estaciones)
p <- p + geom_polygon(aes(x,y),fill=NA,color="black",data=borde)
p <- p + scale_color_gradient(low = "yellow", high = "red")
p <- p + labs(title = "Predicciones para temperaturas máximas", 
                              x = "x",y ="y", color="grados")
p <- p + theme_bw()
p
```


```{r}
p <- ggplot(aes(x,y),data=pred_krig)
p <- p +  geom_point(aes(color=var1.var))
p <- p + geom_point(aes(x,y), data = estaciones)
p <- p + geom_polygon(aes(x,y),fill=NA,color="black",data=borde)
p <- p + scale_color_gradient(low = "yellow", high = "red")
p <- p + labs(title = "Varianza de las predicciones", 
                              x = "x",y ="y",
                              color ="Varianza\n")
p <- p + theme_bw()
p
```









